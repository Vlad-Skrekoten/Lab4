{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdvJ-bKDQqNA"
      },
      "source": [
        "# Робота з Spark SQL\n",
        "## Складна аналітика з Spark SQL\n",
        "\n",
        "> Можна виконати або за допомогою чистого `DataFrame` API, `Pandas on Spark` API, або за допомогою чистих `SQL` запитів. Вибір за вами.\n",
        "\n",
        "Ви розробник у компанії **BikeServe**, яка займається орендою велосипедів/скутерів. У вас є певні місця (\"станції\"), де зберігаються ваші велосипеди. Якщо на станції немає вільних місць для велосипедів (хтось уже зарезервував або взяв велосипед) протягом певного періоду часу (`timeslot`), це означає, що бізнес йде чудово. Однак вам потрібно покращити обслуговування клієнтів, пропонуючи користувачам велосипеди, коли та в тому місці, де це для них найважливіше.\n",
        "\n",
        "Ваше завдання — знайти найбільш важливі (\"критичні\") пари станції та періоду часу `(stationId, timeslot)`, щоб ваш бізнес знав, куди і коли доставити більше велосипедів.\n",
        "\n",
        "Ваш результат має бути відсортований за цією *критичністю* у порядку спадання.\n",
        "\n",
        "Набір даних містить:\n",
        "* `register.csv` містить інформацію з вашої IoT системи моніторингу про кількість використаних і вільних слотів на ваших станціях оренди велосипедів. Кожен рядок відповідає одному запису про ситуацію на одній станції в певний момент часу.\n",
        "\n",
        "    Кожен рядок має такий формат:\n",
        "\n",
        "    ```bash\n",
        "    stationId\\ttimestamp\\tusedslots\\tfreeslots\n",
        "    ```\n",
        "    \n",
        "    де `timestamp` має формат `datetime`.\n",
        "\n",
        "    > Перший рядок файлу містить заголовок.\n",
        "    > Деякі дані в наборі даних пошкоджено через тимчасові збої мережі та/або вашої системи моніторингу. Це означає, що деякі рядки характеризуються \"використаними слотами (used slots) = 0\" і \"вільними слотами (free slots) = 0\". **Ці рядки необхідно відфільтрувати** перед виконанням будь-яких операцій.\n",
        "\n",
        "* `input/stations.csv` містить опис станцій.\n",
        "\n",
        "    Кожен рядок має такий формат:\n",
        "\n",
        "    ```bash\n",
        "    stationId\\tlongitude\\ttitude\\tname\n",
        "    ```\n",
        "    > Перший рядок файлу містить заголовок.\n",
        "\n",
        "### Опис завдання\n",
        "\n",
        "Кожна пара \"день тижня – година\" є \"часовим інтервалом\" (`timeslot`) і пов’язана з усіма показаннями моніторингу, пов’язаними з цією парою, незалежно від дати. Наприклад, часовий інтервал `Wednesday - 17` відповідає всім показанням, зробленим у середу з `17:00:00` до `17:59:59`.\n",
        "\n",
        "Станція $S_i$ знаходиться в критичному стані, якщо кількість вільних слотів дорівнює `0` (всі велосипеди на станції заброньовані).\n",
        "\n",
        "*Критичність* станції $S_i$ у часовому інтервалі $T_j$ визначається як:\n",
        "\n",
        "$$\n",
        "\\frac{\\text{кількість записів із числом вільних слотів, яке дорівнює нулю, для пари}_{\\left(S_i,T_j\\right)}}{\\text{загальна кількість записів для пари}_{\\left(S_i,T_j\\right)}}\n",
        "$$\n",
        "\n",
        "необхідно:\n",
        "* Обчислити значення *критичності* для кожної пари $(S_i, T_j)$.\n",
        "* Вибирати лише пари, у яких значення *критичності* перевищує \"мінімальний поріг критичності\".\n",
        "    * `Мінімальний поріг критичності` має бути параметром конфігурації програми.\n",
        "* Зберегти у вихідній папці вибрані записи, використовуючи файли `csv` (із заголовком). Зберегти лише такі атрибути:\n",
        "    * ідентифікатор станції\n",
        "    * день тижня\n",
        "    * година\n",
        "    * критичність\n",
        "    * довгота станції\n",
        "    * широта станції\n",
        "* Зберегти результати за зменшення критичності. Якщо є два або більше записів, що характеризуються однаковим значенням критичності, то додатково відсортувати по ідентифікатору станції (у порядку зростання). Якщо і станція та сама, то сортувати за днем тижня (за зростанням) і, нарешті, за годиною (за зростанням).\n",
        "\n",
        "### Поради та підказки\n",
        "\n",
        "Мова SQL, доступна в Spark SQL, має низку попередньо визначених функцій, одна з яких, `hour(timestamp)`, може використовуватися в запитах SQL або в перетворенні `selectExpr`, щоб вибрати `hour` з заданого позначка часу. Ще одна цікава функція, `date_format(timestamp,format)`, може бути використана для отримання іншої корисної інформації зі стовпця timestamp. Наприклад, у форматі `EE` можна отримати день тижня.\n",
        "\n",
        "```python\n",
        "new_df= df.selectExpr(\"date_format(timestamp,'EE') as weekday hour(timestamp) as hour\")\n",
        "```\n",
        "\n",
        "Щоб вказати, що роздільником вхідних файлів CSV є спеціальний символ `tab`, установіть параметр роздільника на `\\\\t`, викликавши `.option(\"delimiter\", \"\\\\t\")` під час читання вхідних даних."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QuAvV-SQqNC"
      },
      "source": [
        "## Конфігурація\n",
        "\n",
        "- `number_cores`: Кількість ядер, виділених під Spark\n",
        "- `memory_gb`: Обʼєм оперативної памʼяті, виділеної під Spark (в Гб)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxgznuJ6QqND"
      },
      "outputs": [],
      "source": [
        "number_cores = 2\n",
        "memory_gb = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2MVs3LfQqND"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "spark = (SparkSession\n",
        "    .builder\n",
        "    .appName('Spark Bikes')\n",
        "    .master(f\"local[{number_cores}]\")\n",
        "    .config(\"spark.driver.memory\", f\"{memory_gb}g\")\n",
        "    .getOrCreate())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaadZYHTQqNE"
      },
      "source": [
        "## Рішення\n",
        "Прочитайте вміст вхідного файлу `register.csv` і збережіть його у DataFrame.\n",
        "\n",
        "Вхідний файл має заголовок.\n",
        "\n",
        "Схема даних:\n",
        "* station: integer (nullable = true)\n",
        "* timestamp: timestamp (nullable = true)\n",
        "* used_slots: integer (nullable = true)\n",
        "* free_slots: integer (nullable = true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TsXacWOQqNE"
      },
      "outputs": [],
      "source": [
        "input_file_path = \"register.csv\"\n",
        "df = spark.read.csv(input_file_path, header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oTForhTQqNE"
      },
      "source": [
        "Видаліть рядки де одночасно `free_slots = 0` та `used_slots = 0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBM2S08jQqNE"
      },
      "outputs": [],
      "source": [
        "df_filtered = df.filter((df['free_slots'] != 0) | (df['used_slots'] != 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgzQ2mUKQqNE"
      },
      "source": [
        "Нам потрібен логічний маркер, щоб побачити, заповнена станція чи ні. Це можна зробити за допомогою UDF під назвою `full(free_slots: int)`, яка повертає\n",
        "* 1, якщо `free_slots` дорівнює 0\n",
        "* 0, якщо `free_slots` більше 0\n",
        "\n",
        "> Якщо ви використовуєте Pandas on Spark API, то треба самостійно застосувати цю функцію (або переписати її)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PADU-EG9QqNE"
      },
      "outputs": [],
      "source": [
        "def full_function(free_slots: int) -> int:\n",
        "    if free_slots==0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "spark.udf.register(\"full\", full_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDmwPMUJQqNF"
      },
      "source": [
        "Створіть DataFrame з такою схемою:\n",
        "* station: integer (nullable = true)\n",
        "* dayofweek: string (nullable = true)\n",
        "* hour: integer (nullable = true)\n",
        "* fullstatus: integer (nullable = true) - 1 = full, 0 = non-full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNF_sgdoQqNF"
      },
      "outputs": [],
      "source": [
        "df = df.withColumn(\"fullstatus\", udf(lambda free_slots: full_function(free_slots), IntegerType())(df[\"free_slots\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLwaGeo9QqNF"
      },
      "source": [
        "Визначте одну групу для кожної комбінації `(station, dayofweek, hour)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xo54na_QqNF"
      },
      "outputs": [],
      "source": [
        "grouped_df = df.groupBy(\"station\", \"dayofweek\", \"hour\").count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7AIck2XQqNF"
      },
      "source": [
        "Обчисліть \"критичність\" для кожної групи `(station, dayofweek, hour)`, тобто для кожної пари `(station, timeslot)`.\n",
        "\n",
        "Критичність дорівнює середньому `fullStatus`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYt7HJHQQqNF"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "criticality_df = df.groupBy(\"station\", \"dayofweek\", \"hour\").agg(avg(\"fullstatus\").alias(\"criticality\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzv99WiWQqNG"
      },
      "source": [
        "Виберіть лише рядки з `criticality > threshold`\n",
        "\n",
        "> `threshold` є деякою бізнес-вимогою, тому візьміть випадкове число від `0.1` до `0.5`, яке вам подобається :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_dcFV30QqNG"
      },
      "outputs": [],
      "source": [
        "threshold = 0.25\n",
        "\n",
        "selected_rows = criticality_df.filter(criticality_df[\"criticality\"] > threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP5HS_GGQqNG"
      },
      "source": [
        "Прочитайте вміст вхідного файлу `stations.csv` і збережіть його у DataFrame.\n",
        "\n",
        "Вхідний файл має заголовок.\n",
        "\n",
        "Схема даних:\n",
        "* id: integer (nullable = true)\n",
        "* longitude: double (nullable = true)\n",
        "* latitude: double (nullable = true)\n",
        "* name: string (nullable = true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSXxJrIjQqNG"
      },
      "outputs": [],
      "source": [
        "input_file_path = \"stations.csv\"\n",
        "stations_df = spark.read.csv(input_file_path, header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hv25AcGQqNG"
      },
      "source": [
        "Об’єднайте (`JOIN`) вибрані критичні часові інтервали з таблицею станцій, щоб отримати координати станцій"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhqD7_cHQqNG"
      },
      "outputs": [],
      "source": [
        "joined_df = selected_criticality_df.join(stations_df, selected_criticality_df[\"station\"] == stations_df[\"id\"], \"inner\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh5juFcWQqNG"
      },
      "source": [
        "Відсортуйте вміст DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OfjWCtjQqNG"
      },
      "outputs": [],
      "source": [
        "sorted_df = joined_df.sort([\"criticality\", \"station\"], ascending=[False, True])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpC1tPzfQqNG"
      },
      "source": [
        "Write to file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJuY0eZCQqNH"
      },
      "outputs": [],
      "source": [
        "output_file_path = \"sorted_data.csv\"\n",
        "\n",
        "sorted_df.write.csv(output_file_path, header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqfIrnvtQqNH"
      },
      "source": [
        "## Зупинка Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj_s-c69QqNH"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}